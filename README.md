# Sara AI - Commerce Support Advisor

[![React Version](https://img.shields.io/badge/react-18.2.0-8B4513.svg)](https://reactjs.org/)
[![TypeScript](https://img.shields.io/badge/typescript-5.3.0-A0522D.svg)](https://www.typescriptlang.org/)
[![Node.js](https://img.shields.io/badge/node.js-20.0.0-8B0000.svg)](https://nodejs.org/)
[![Python](https://img.shields.io/badge/python-3.12.0-2F4F4F.svg)](https://www.python.org/)
[![Build Status](https://img.shields.io/badge/build-passing-228B22.svg)](https://github.com/TensorScholar/commerce-support-advisor)
[![Coverage](https://img.shields.io/badge/coverage-95%25-8B4513.svg)](https://github.com/TensorScholar/commerce-support-advisor)
[![License: MIT](https://img.shields.io/badge/License-MIT-696969.svg)](https://opensource.org/licenses/MIT)
[![Production Ready](https://img.shields.io/badge/production-ready-2F1B14.svg)](https://github.com/TensorScholar/commerce-support-advisor)

**Sara AI** is a production-grade, intelligent commerce support advisor designed specifically for Persian language e-commerce platforms. It provides real-time product recommendations, customer support, and sales assistance through an embeddable chat widget with advanced RAG (Retrieval-Augmented Generation) capabilities.

This project demonstrates modern software engineering principles through a sophisticated **microservices architecture**, multi-layered **performance caching strategy**, and proactive resilience through **circuit breaker patterns**.

## üèóÔ∏è System Architecture

The system is architected using **microservices principles**, ensuring high availability, scalability, and maintainability across multiple technology stacks.

```mermaid
graph TD
    subgraph Frontend Layer
        A[React Widget - Shadow DOM]
        B[TypeScript + Vite]
        C[Zustand State Management]
    end

    subgraph API Gateway Layer
        D[Node.js + Express]
        E[Rate Limiting + Validation]
        F[Circuit Breaker + Caching]
    end

    subgraph Intelligence Layer
        G[Python RAG Engine]
        H[OpenAI GPT-4 + Claude]
        I[Pinecone Vector Search]
    end

    subgraph Data Layer
        J[MongoDB - Conversations]
        K[Redis - Multi-tier Cache]
        L[Elasticsearch - Keyword Search]
    end

    A --> D
    D --> G
    G --> I
    G --> H
    D --> J
    D --> K
    G --> L

    style Frontend fill:#8B4513,stroke:#654321,stroke-width:2px,color:#fff
    style API fill:#A0522D,stroke:#8B4513,stroke-width:2px,color:#fff
    style Intelligence fill:#2F4F4F,stroke:#1C3A3A,stroke-width:2px,color:#fff
    style Data fill:#8B0000,stroke:#660000,stroke-width:2px,color:#fff
```

### Architectural Layers

- **Frontend Layer**: React-based embeddable widget with Shadow DOM isolation, real-time WebSocket communication, and Persian language support.

- **API Gateway Layer**: Node.js Express service handling rate limiting, request validation, response caching, and circuit breaker patterns for resilience.

- **Intelligence Layer**: Python FastAPI service implementing RAG pipeline with hybrid search, LLM integration, and response validation.

- **Data Layer**: Multi-database architecture with MongoDB for conversations, Redis for caching, Pinecone for vectors, and Elasticsearch for keyword search.

## üöÄ Key Features

### Multi-Layered Caching Strategy

1. **L1 Cache: In-Memory (LRU)**
   - Microsecond-level access to frequently requested data
   - Local to each process, fastest available tier
   - 5-minute TTL for optimal performance

2. **L2 Cache: Redis (Distributed)**
   - 1-hour TTL for session data and common queries
   - Ensures cache coherency in scaled-out environments
   - Stores conversation context and product recommendations

3. **L3 Cache: Database (Persistent)**
   - Long-term storage for conversation history
   - MongoDB for conversation persistence
   - Pinecone for vector embeddings

### Advanced RAG Pipeline

The system implements a sophisticated Retrieval-Augmented Generation pipeline:

- **Persian Text Normalization**: Hazm library for accurate Persian text processing
- **Hybrid Search**: Combines semantic vector search with BM25 keyword search
- **Response Validation**: Ensures all responses are traceable to source data
- **Sales Strategy Injection**: Rule-based post-processing for product recommendations

### Production-Grade Resilience

- **Circuit Breaker Pattern**: Automatic failure detection and recovery
- **Rate Limiting**: Token bucket algorithm with Redis backend
- **Request Deduplication**: Prevents duplicate API calls
- **Graceful Degradation**: Fallback mechanisms for all external dependencies

### Real-Time Communication

- **WebSocket Support**: Real-time message streaming
- **Shadow DOM**: CSS isolation for embeddable widget
- **Lazy Loading**: Route-based code splitting for optimal performance
- **Offline Support**: Service worker for offline functionality

## üìã Prerequisites

- Node.js 20+
- Python 3.12+
- PNPM 8+
- Docker & Docker Compose (optional)

## üõ†Ô∏è Installation & Configuration

1. **Clone the repository:**
   ```bash
   git clone https://github.com/TensorScholar/commerce-support-advisor.git
   cd commerce-support-advisor
   ```

2. **Install dependencies:**
   ```bash
   pnpm install
   ```

3. **Configure environment:**
   Create a `.env` file with your API keys and configuration:
   ```env
   # LLM Configuration
   OPENAI_API_KEY=sk-your-openai-api-key-here
   ANTHROPIC_API_KEY=sk-ant-your-anthropic-api-key-here
   
   # Vector Database
   PINECONE_API_KEY=your-pinecone-api-key-here
   PINECONE_ENVIRONMENT=us-west1-gcp
   PINECONE_INDEX_NAME=sara-products
   
   # Databases
   MONGODB_URI=mongodb://localhost:27017/sara
   REDIS_URL=redis://localhost:6379
   
   # API Gateway
   PORT=3000
   NODE_ENV=development
   RATE_LIMIT_WINDOW_MS=60000
   RATE_LIMIT_MAX_REQUESTS=100
   
   # RAG Engine
   RAG_PORT=8000
   EMBEDDING_MODEL=text-embedding-3-large
   LLM_MODEL=gpt-4-turbo-2024-04-09
   LLM_TEMPERATURE=0.3
   MAX_TOKENS=600
   RETRIEVAL_TOP_K=5
   CONFIDENCE_THRESHOLD=0.85
   
   # Monitoring
   SENTRY_DSN=https://your-sentry-dsn-here
   LOG_LEVEL=info
   
   # Widget Configuration
   WIDGET_API_URL=http://localhost:3000/api/v1
   WIDGET_WS_URL=ws://localhost:3000/ws
   ```

## üöÄ Running the Service

### Development Mode
```bash
# Start all services
pnpm run dev

# Start individual services
pnpm run dev --filter=@sara-ai/widget
pnpm run dev --filter=@sara-ai/api-gateway
pnpm run dev --filter=@sara-ai/rag-engine
```

### Docker Compose
```bash
# Development environment
pnpm run docker:up

# Production environment
docker-compose -f infrastructure/docker/docker-compose.prod.yml up -d
```

### Production Deployment
```bash
# Build all services
pnpm run build

# Start production services
pnpm run start
```

## üß™ Testing

The project maintains a comprehensive testing suite:

```bash
# Run all tests with coverage
pnpm run test

# Run specific test categories
pnpm run test --filter=@sara-ai/widget        # Widget tests
pnpm run test --filter=@sara-ai/api-gateway   # API Gateway tests
pnpm run test --filter=@sara-ai/rag-engine    # RAG Engine tests

# Run E2E tests
pnpm run test:e2e

# Run performance tests
pnpm run test:performance
```

## üìä Performance Benchmarks

The system includes comprehensive performance monitoring:

- **Widget Load Time**: < 2s (95th percentile)
- **API Response Time**: < 2.5s (95th percentile)
- **RAG Processing Time**: < 3s (95th percentile)
- **Cache Hit Rate**: > 85% (L1 + L2 combined)
- **Memory Usage**: Optimized with automatic garbage collection
- **Concurrent Users**: 500+ supported

## üîß Configuration

The system supports extensive configuration through environment variables:

- **API Keys**: OpenAI, Anthropic, Pinecone, MongoDB, Redis
- **Performance Tuning**: Cache TTL, connection pools, timeouts
- **Monitoring**: Metrics collection, alerting thresholds
- **Security**: Rate limiting, input validation, CORS settings

## üìà Monitoring & Observability

- **Metrics Collection**: Prometheus-compatible metrics
- **Distributed Tracing**: OpenTelemetry integration
- **Error Tracking**: Sentry integration for real-time error monitoring
- **Health Checks**: Comprehensive health check endpoints
- **Logging**: Structured JSON logging with Winston

## üöÄ Production Features

### High Availability
- **Multi-region deployment** support
- **Auto-scaling** based on CPU and memory metrics
- **Load balancing** with health checks
- **Database clustering** for high availability

### Security
- **Input validation** with Zod schemas
- **Rate limiting** per IP and user
- **CORS** configuration for cross-origin requests
- **Helmet** security headers
- **Request sanitization** and validation

### Performance
- **CDN integration** for static assets
- **Database indexing** for optimal query performance
- **Connection pooling** for database connections
- **Response compression** with gzip

## ü§ù Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üë®‚Äçüíª Author

**Mohammad Atashi** - mohammadaliatashi@icloud.com

---

**Sara AI** - Intelligent Commerce Support for Persian E-commerce Platforms